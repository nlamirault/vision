# Telegraf configuration

# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared plugins.

# Even if a plugin has no configuration, it must be declared in here
# to be active. Declaring a plugin means just specifying the name
# as a section with no variables. To deactivate a plugin, comment
# out the name and any variables.

# Use 'telegraf -config telegraf.toml -test' to see what metrics a config
# file would generate.

# One rule that plugins conform to is wherever a connection string
# can be passed, the values '' and 'localhost' are treated specially.
# They indicate to the plugin to use their own builtin configuration to
# connect to the local system.

# NOTE: The configuration has a few required parameters. They are marked
# with 'required'. Be sure to edit those to make this configuration work.

# Tags can also be specified via a normal map, but only one form at a time:
[tags]
  # dc = "us-east-1"

# Configuration for telegraf agent
[agent]
  # Default data collection interval for all plugins
  interval = "10s"
  # Rounds collection interval to 'interval'
  # ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  # Default data flushing interval for all outputs. You should not set this below
  # interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "10s"
  # Jitter the flush interval by a random amount. This is primarily to avoid
  # large write spikes for users running a large number of telegraf instances.
  # ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  # Run telegraf in debug mode
  debug = false
  # Override default hostname, if empty use os.Hostname()
  hostname = ""


###############################################################################
#                                  OUTPUTS                                    #
###############################################################################

[outputs]

# Configuration for Amon Server to send metrics to.
# [[outputs.amon]]
#   # Amon Server Key
#   server_key = "my-server-key" # required.

#   # Amon Instance URL
#   amon_instance = "https://youramoninstance" # required

#   # Connection timeout.
#   # timeout = "5s"

# # Configuration for the AMQP server to send metrics to
# [[outputs.amqp]]
#   # AMQP url
#   url = "amqp://localhost:5672/influxdb"
#   # AMQP exchange
#   exchange = "telegraf"
#   # Telegraf tag to use as a routing key
#   #  ie, if this tag exists, it's value will be used as the routing key
#   routing_tag = "host"

#   # InfluxDB retention policy
#   #retention_policy = "default"
#   # InfluxDB database
#   #database = "telegraf"
#   # InfluxDB precision
#   #precision = "s"

# # Configuration for DataDog API to send metrics to.
# [[outputs.datadog]]
#   # Datadog API key
#   apikey = "my-secret-key" # required.

#   # Connection timeout.
#   # timeout = "5s"

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
  # The full HTTP or UDP endpoint URL for your InfluxDB instance.
  # Multiple urls can be specified but it is assumed that they are part of the same
  # cluster, this means that only ONE of the urls will be written to each interval.
  # urls = ["udp://localhost:8089"] # UDP endpoint example
  urls = ["http://localhost:8086"] # required
  # The target database for metrics (telegraf will create it if not exists)
  database = "telegraf" # required
  # Precision of writes, valid values are n, u, ms, s, m, and h
  # note: using second precision greatly helps InfluxDB compression
  precision = "s"

  # Connection timeout (for the connection with InfluxDB), formatted as a string.
  # If not provided, will default to 0 (no timeout)
  # timeout = "5s"
  # username = "telegraf"
  # password = "metricsmetricsmetricsmetrics"
  # Set the user agent for HTTP POSTs (can be useful for log differentiation)
  # user_agent = "telegraf"
  # Set UDP payload size, defaults to InfluxDB UDP Client default (512 bytes)
  # udp_payload = 512

# Configuration for the Kafka server to send metrics to
# [[outputs.kafka]]
#   # URLs of kafka brokers
#   brokers = ["localhost:9092"]
#   # Kafka topic for producer messages
#   topic = "telegraf"
#   # Telegraf tag to use as a routing key
#   #  ie, if this tag exists, it's value will be used as the routing key
#   routing_tag = "host"

# # Configuration for Librato API to send metrics to.
# [[outputs.librato]]
#   # Librator API Docs
#   # http://dev.librato.com/v1/metrics-authentication

#   # Librato API user
#   api_user = "telegraf@influxdb.com" # required.

#   # Librato API token
#   api_token = "my-secret-token" # required.

#   # Tag Field to populate source attribute (optional)
#   # This is typically the _hostname_ from which the metric was obtained.
#   source_tag = "hostname"

#   # Connection timeout.
#   # timeout = "5s"

# # Configuration for MQTT server to send metrics to
# [[outputs.mqtt]]
#   servers = ["localhost:1883"] # required.

#   # MQTT outputs send metrics to this topic format
#   #    "<topic_prefix>/host/<hostname>/<pluginname>/"
#   #   ex: prefix/host/web01.example.com/mem/available
#   # topic_prefix = "prefix"

#   # username and password to connect MQTT server.
#   # username = "telegraf"
#   # password = "metricsmetricsmetricsmetrics"

# # Send telegraf measurements to NSQD
# [[outputs.nsq]]
#   # Location of nsqd instance listening on TCP
#   server = "localhost:4150"
#   # NSQ topic for producer messages
#   topic = "telegraf"

# # Configuration for OpenTSDB server to send metrics to
# [[outputs.opentsdb]]
#   # prefix for metrics keys
#   prefix = "my.specific.prefix."

#   ## Telnet Mode ##
#   # DNS name of the OpenTSDB server in telnet mode
#   host = "opentsdb.example.com"

#   # Port of the OpenTSDB server in telnet mode
#   port = 4242

#   # Debug true - Prints OpenTSDB communication
#   debug = false

# # Configuration for the Prometheus client to spawn
# [[outputs.prometheus_client]]
#   # Address to listen on
#   # listen = ":9126"

# # Configuration for the Riemann server to send metrics to
# [[outputs.riemann]]
#   # URL of server
#   url = "localhost:5555"
#   # transport protocol to use either tcp or udp
#   transport = "tcp"


###############################################################################
#                                  PLUGINS                                    #
###############################################################################

[plugins]

# Inserts sine and cosine waves for demonstration purposes
# [[plugins.Trig]]
#   # Set the amplitude
#   amplitude = 10.0

# # Read stats from an aerospike server
# [[plugins.aerospike]]
#   # Aerospike servers to connect to (with port)
#   # Default: servers = ["localhost:3000"]
#   #
#   # This plugin will query all namespaces the aerospike
#   # server has configured and get stats for them.
#   servers = ["localhost:3000"]

# # Read Apache status information (mod_status)
# [[plugins.apache]]
#   # An array of Apache status URI to gather stats.
#   urls = ["http://localhost/server-status?auto"]

# # Read metrics of bcache from stats_total and dirty_data
# [[plugins.bcache]]
#   # Bcache sets path
#   # If not specified, then default is:
#   # bcachePath = "/sys/fs/bcache"
#   #
#   # By default, telegraf gather stats for all bcache devices
#   # Setting devices will restrict the stats to the specified
#   # bcache devices.
#   # bcacheDevs = ["bcache0", ...]

# Read metrics about cpu usage
[[plugins.cpu]]
  # Whether to report per-cpu stats or not
  percpu = true
  # Whether to report total system cpu stats or not
  totalcpu = true
  # Comment this line if you want the raw CPU time metrics
  drop = ["cpu_time"]

# Read metrics about disk usage by mount point
[[plugins.disk]]
  # By default, telegraf gather stats for all mountpoints.
  # Setting mountpoints will restrict the stats to the specified mountpoints.
  # Mountpoints=["/"]

# Read metrics from one or many disque servers
[[plugins.disque]]
  # An array of URI to gather stats about. Specify an ip or hostname
  # with optional port and password. ie disque://localhost, disque://10.10.3.33:18832,
  # 10.0.0.1:10000, etc.
  #
  # If no servers are specified, then localhost is used as the host.
  servers = ["localhost"]

# Read metrics about docker containers
[[plugins.docker]]
  # no configuration

# Read stats from one or more Elasticsearch servers or clusters
[[plugins.elasticsearch]]
  # specify a list of one or more Elasticsearch servers
  servers = ["http://localhost:9200"]

  # set local to false when you want to read the indices stats from all nodes
  # within the cluster
  local = true

  # set cluster_health to true when you want to also obtain cluster level stats
  cluster_health = false

# Read flattened metrics from one or more commands that output JSON to stdout
# [[plugins.exec]]
#   # specify commands via an array of tables
#   [[plugins.exec.commands]]
#   # the command to run
#   command = "/usr/bin/mycollector --foo=bar"

#   # name of the command (used as a prefix for measurements)
#   name = "mycollector"

#   # Only run this command if it has been at least this many
#   # seconds since it last ran
#   interval = 10

# Read metrics of haproxy, via socket or csv stats page
# [[plugins.haproxy]]
#   # An array of address to gather stats about. Specify an ip on hostname
#   # with optional port. ie localhost, 10.10.3.33:1936, etc.
#   #
#   # If no servers are specified, then default to 127.0.0.1:1936
#   servers = ["http://myhaproxy.com:1936", "http://anotherhaproxy.com:1936"]
#   # Or you can also use local socket(not work yet)
#   # servers = ["socket:/run/haproxy/admin.sock"]

# Read flattened metrics from one or more JSON HTTP endpoints
# [[plugins.httpjson]]
#   # Specify services via an array of tables
#   [[plugins.httpjson.services]]

#     # a name for the service being polled
#     name = "webserver_stats"

#     # URL of each server in the service's cluster
#     servers = [
#       "http://localhost:9999/stats/",
#       "http://localhost:9998/stats/",
#     ]

#     # HTTP method to use (case-sensitive)
#     method = "GET"

#     # List of tag names to extract from top-level of JSON server response
#     # tag_keys = [
#     #   "my_tag_1",
#     #   "my_tag_2"
#     # ]

#     # HTTP parameters (all values must be strings)
#     [plugins.httpjson.services.parameters]
#       event_type = "cpu_spike"
#       threshold = "0.75"

# Read metrics about disk IO by device
[[plugins.io]]
  # By default, telegraf will gather stats for all devices including
  # disk partitions.
  # Setting devices will restrict the stats to the specified devcies.
  # devices = ["sda","sdb"]
  # Uncomment the following line if you do not need disk serial numbers.
  # skip_serial_number = true

# # Read JMX metrics through Jolokia
# [[plugins.jolokia]]
#   # This is the context root used to compose the jolokia url
#   context = "/jolokia/read"

#   # Tags added to each measurements
#   [jolokia.tags]
#     group = "as"

#   # List of servers exposing jolokia read service
#   [[plugins.jolokia.servers]]
#     name = "stable"
#     host = "192.168.103.2"
#     port = "8180"
#     # username = "myuser"
#     # password = "mypassword"

#   # List of metrics collected on above servers
#   # Each metric consists in a name, a jmx path and either a pass or drop slice attributes
#   # This collect all heap memory usage metrics
#   [[plugins.jolokia.metrics]]
#     name = "heap_memory_usage"
#     jmx  = "/java.lang:type=Memory/HeapMemoryUsage"


#   # This drops the 'committed' value from Eden space measurement
#   [[plugins.jolokia.metrics]]
#     name = "memory_eden"
#     jmx  = "/java.lang:type=MemoryPool,name=PS Eden Space/Usage"
#     drop = [ "committed" ]


#   # This passes only DaemonThreadCount and ThreadCount
#   [[plugins.jolokia.metrics]]
#     name = "heap_threads"
#     jmx  = "/java.lang:type=Threading"
#     pass = [
#       "DaemonThreadCount",
#       "ThreadCount"
#     ]

# # Read metrics from a LeoFS Server via SNMP
# [[plugins.leofs]]
#   # An array of URI to gather stats about LeoFS.
#   # Specify an ip or hostname with port. ie 127.0.0.1:4020
#   #
#   # If no servers are specified, then 127.0.0.1 is used as the host and 4020 as the port.
#   servers = ["127.0.0.1:4021"]

# # Read metrics from local Lustre service on OST, MDS
# [[plugins.lustre2]]
#   # An array of /proc globs to search for Lustre stats
#   # If not specified, the default will work on Lustre 2.5.x
#   #
#   # ost_procfiles = ["/proc/fs/lustre/obdfilter/*/stats", "/proc/fs/lustre/osd-ldiskfs/*/stats"]
#   # mds_procfiles = ["/proc/fs/lustre/mdt/*/md_stats"]

# # Gathers metrics from the /3.0/reports MailChimp API
# [[plugins.mailchimp]]
#   # MailChimp API key
#   # get from https://admin.mailchimp.com/account/api/
#   api_key = "" # required
#   # Reports for campaigns sent more than days_old ago will not be collected.
#   # 0 means collect all.
#   days_old = 0
#   # Campaign ID to get, if empty gets all campaigns, this option overrides days_old
#   # campaign_id = ""

# Read metrics about memory usage
[[plugins.mem]]
  # no configuration

# # Read metrics from one or many memcached servers
# [[plugins.memcached]]
#   # An array of address to gather stats about. Specify an ip on hostname
#   # with optional port. ie localhost, 10.0.0.1:11211, etc.
#   #
#   # If no servers are specified, then localhost is used as the host.
#   servers = ["localhost:11211"]
#   # unix_sockets = ["/var/run/memcached.sock"]

# # Read metrics from one or many MongoDB servers
# [[plugins.mongodb]]
#   # An array of URI to gather stats about. Specify an ip or hostname
#   # with optional port add password. ie mongodb://user:auth_key@10.10.3.30:27017,
#   # mongodb://10.10.3.33:18832, 10.0.0.1:10000, etc.
#   #
#   # If no servers are specified, then 127.0.0.1 is used as the host and 27107 as the port.
#   servers = ["127.0.0.1:27017"]

# # Read metrics from one or many mysql servers
# [[plugins.mysql]]
#   # specify servers via a url matching:
#   #  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify]]
#   #  see https://github.com/go-sql-driver/mysql#dsn-data-source-name
#   #  e.g.
#   #    root:passwd@tcp(127.0.0.1:3306)/?tls=false
#   #    root@tcp(127.0.0.1:3306)/?tls=false
#   #
#   # If no servers are specified, then localhost is used as the host.
#   servers = ["tcp(127.0.0.1:3306)/"]

# Read metrics about network interface usage
[[plugins.net]]
  # By default, telegraf gathers stats from any up interface (excluding loopback)
  # Setting interfaces will tell it to gather these explicit interfaces,
  # regardless of status.
  #
  # interfaces = ["eth0", ... ]

# Read metrics about TCP status such as established, time wait etc and UDP sockets counts.
[[plugins.netstat]]
  # no configuration

# # Read Nginx's basic status information (ngx_http_stub_status_module)
# [[plugins.nginx]]
#   # An array of Nginx stub_status URI to gather stats.
#   urls = ["http://localhost/status"]

# # Read metrics of phpfpm, via HTTP status page or socket(pending)
# [[plugins.phpfpm]]
#   # An array of addresses to gather stats about. Specify an ip or hostname
#   # with optional port and path.
#   #
#   # Plugin can be configured in three modes (both can be used):
#   #   - http: the URL must start with http:// or https://, ex:
#   #       "http://localhost/status"
#   #       "http://192.168.130.1/status?full"
#   #   - unixsocket: path to fpm socket, ex:
#   #       "/var/run/php5-fpm.sock"
#   #       "192.168.10.10:/var/run/php5-fpm-www2.sock"
#   #   - fcgi: the URL mush start with fcgi:// or cgi://, and port must present, ex:
#   #       "fcgi://10.0.0.12:9000/status"
#   #       "cgi://10.0.10.12:9001/status"
#   #
#   # If no servers are specified, then default to 127.0.0.1/server-status
#   urls = ["http://localhost/status"]

# Ping given url(s) and return statistics
[[plugins.ping]]
  # urls to ping
  urls = ["www.google.com"] # required
  # number of pings to send (ping -c <COUNT>)
  count = 1 # required
  # interval, in s, at which to ping. 0 == default (ping -i <PING_INTERVAL>)
  ping_interval = 0.0
  # ping timeout, in s. 0 == no timeout (ping -t <TIMEOUT>)
  timeout = 0.0
  # interface to send ping from (ping -I <INTERFACE>)
  interface = ""

# # Read metrics from one or many postgresql servers
# [[plugins.postgresql]]
#   # specify servers via an array of tables
#   [[plugins.postgresql.servers]]

#   # specify address via a url matching:
#   #   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]
#   # or a simple string:
#   #   host=localhost user=pqotest password=... sslmode=... dbname=app_production
#   #
#   # All connection parameters are optional. By default, the host is localhost
#   # and the user is the currently running user. For localhost, we default
#   # to sslmode=disable as well.
#   #
#   # Without the dbname parameter, the driver will default to a database
#   # with the same name as the user. This dbname is just for instantiating a
#   # connection with the server and doesn't restrict the databases we are trying
#   # to grab metrics for.
#   #

#   address = "sslmode=disable"

#   # A list of databases to pull metrics about. If not specified, metrics for all
#   # databases are gathered.

#   # databases = ["app_production", "blah_testing"]

#   # [[plugins.postgresql.servers]]
#   # address = "influx@remoteserver"

# Monitor process cpu and memory usage
# [[plugins.procstat]]
#   [[plugins.procstat.specifications]]
#   prefix = "" # optional string to prefix measurements
#   # Must specify one of: pid_file, exe, or pattern
#   # PID file to monitor process
#   pid_file = "/var/run/nginx.pid"
#   # executable name (ie, pgrep <exe>)
#   # exe = "nginx"
#   # pattern as argument for pgrep (ie, pgrep -f <pattern>)
#   # pattern = "nginx"

# # Read metrics from one or many prometheus clients
# [[plugins.prometheus]]
#   # An array of urls to scrape metrics from.
#   urls = ["http://localhost:9100/metrics"]

# # Reads last_run_summary.yaml file and converts to measurments
# [[plugins.puppetagent]]
#   # Location of puppet last run summary file
#   location = "/var/lib/puppet/state/last_run_summary.yaml"

# # Read metrics from one or many RabbitMQ servers via the management API
# [[plugins.rabbitmq]]
#   # Specify servers via an array of tables
#   [[plugins.rabbitmq.servers]]
#   # name = "rmq-server-1" # optional tag
#   # url = "http://localhost:15672"
#   # username = "guest"
#   # password = "guest"

#   # A list of nodes to pull metrics about. If not specified, metrics for
#   # all nodes are gathered.
#   # nodes = ["rabbit@node1", "rabbit@node2"]

# # Read metrics from one or many redis servers
# [[plugins.redis]]
#   # specify servers via a url matching:
#   #  [protocol://][:password]@address[:port]
#   #  e.g.
#   #    tcp://localhost:6379
#   #    tcp://:password@192.168.99.100
#   #
#   # If no servers are specified, then localhost is used as the host.
#   # If no port is specified, 6379 is used
#   servers = ["tcp://localhost:6379"]

# # Read metrics from one or many RethinkDB servers
# [[plugins.rethinkdb]]
#   # An array of URI to gather stats about. Specify an ip or hostname
#   # with optional port add password. ie rethinkdb://user:auth_key@10.10.3.30:28105,
#   # rethinkdb://10.10.3.33:18832, 10.0.0.1:10000, etc.
#   #
#   # If no servers are specified, then 127.0.0.1 is used as the host and 28015 as the port.
#   servers = ["127.0.0.1:28015"]

# Read metrics about swap memory usage
[[plugins.swap]]
  # no configuration

# Read metrics about system load & uptime
[[plugins.system]]
  # no configuration

# # Read Twemproxy stats data
# [[plugins.twemproxy]]
#   [[plugins.twemproxy.instances]]
#     # Twemproxy stats address and port (no scheme)
#     addr = "localhost:22222"
#     # Monitor pool name
#     pools = ["redis_pool", "mc_pool"]

# # Read metrics of ZFS from arcstats, zfetchstats and vdev_cache_stats
# [[plugins.zfs]]
#   # ZFS kstat path
#   # If not specified, then default is:
#   # kstatPath = "/proc/spl/kstat/zfs"
#   #
#   # By default, telegraf gather all zfs stats
#   # If not specified, then default is:
#   # kstatMetrics = ["arcstats", "zfetchstats", "vdev_cache_stats"]

# # Reads 'mntr' stats from one or many zookeeper servers
# [[plugins.zookeeper]]
#   # An array of address to gather stats about. Specify an ip or hostname
#   # with port. ie localhost:2181, 10.0.0.1:2181, etc.

#   # If no servers are specified, then localhost is used as the host.
#   # If no port is specified, 2181 is used
#   servers = [":2181"]


###############################################################################
#                              SERVICE PLUGINS                                #
###############################################################################

# # Statsd Server
# [[plugins.statsd]]
#   # Address and port to host UDP listener on
#   service_address = ":8125"
#   # Delete gauges every interval (default=false)
#   delete_gauges = false
#   # Delete counters every interval (default=false)
#   delete_counters = false
#   # Delete sets every interval (default=false)
#   delete_sets = false
#   # Delete timings & histograms every interval (default=true)
#   delete_timings = true
#   # Percentiles to calculate for timing & histogram stats
#   percentiles = [90]

#   # templates = [
#   #     "cpu.* measurement*"
#   # ]

#   # Number of UDP messages allowed to queue up, once filled,
#   # the statsd server will start dropping packets
#   allowed_pending_messages = 10000

#   # Number of timing/histogram values to track per-measurement in the
#   # calculation of percentiles. Raising this limit increases the accuracy
#   # of percentiles but also increases the memory usage and cpu time.
#   percentile_limit = 1000

# # Read line-protocol metrics from Kafka topic(s)
# [[plugins.kafka_consumer]]
#   # topic(s) to consume
#   topics = ["telegraf"]
#   # an array of Zookeeper connection strings
#   zookeeper_peers = ["localhost:2181"]
#   # the name of the consumer group
#   consumer_group = "telegraf_metrics_consumers"
#   # Maximum number of points to buffer between collection intervals
#   point_buffer = 100000
#   # Offset (must be either "oldest" or "newest")
#   offset = "oldest"
